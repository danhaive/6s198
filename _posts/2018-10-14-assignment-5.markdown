---
layout: post
title:  "Assignment 5"
date:   2018-10-14 23:27:01 -0400
categories: assignments
---

## 1.2.1 
The second FC layer is required because the network must output 28x28x1 images, 
which are multi-dimensional arrays of 784 values. The Reshape layer thus needs 784 values to output 28x28x1 tensors.

## 1.2.2
![img]({{ site.baseurl }}/assets/assignment-6/gan-fc.PNG)

.PNG)  
## 1.2.3 
The performance of the FC architecture is much worse than for the convolutional network. In particular, the GAN requires 
more iterations to reach reasonable results. This is consistent with the 
superior performance of CNNs for image recognition, which was observed in previous assignments. In addition, because in 
the FC model there is no parameter sharing in the form on convolutional filters, samples generated by the FC model are noisier.

## 1.3.1
### ConvNet with greater generator learning rate
* Generator learning rate = 0.05 
* Discriminator learning rate = 0.01
This model is able to generate somewhat reasonable 1's, 4's, 6's, 7's, 8's, and 9's. It does look like there is mode collaspse around 1 as most 
randomly generated samples are 1's.
![img]({{ site.baseurl }}/assets/assignment-6/gan-cnn-1.PNG)

### ConvNet with greater discriminator learning rate
* Generator learning rate = 0.01 
* Discriminator learning rate = 0.05
This model is able to generate 1's only, with a stronger mode collapse than for the first model.
![img]({{ site.baseurl }}/assets/assignment-6/gan-cnn-2.PNG)

### CNN with added convolutional layers
For the last model, I added one convolutional layer to the discriminator, and I increased the number of filters to 50 for the first 
convolutional layer of the generator. After more than 100,000 iterations, the model is capable of generating somewhat reasonable digits, and 
does not seem to be prone to mode collapse. However, it does generate lots of junk samples as well.
![img]({{ site.baseurl }}/assets/assignment-6/gan-cnn-3.PNG)

## 1.3.2
When the discriminator learning rate was greater, the model collapsed completely on mode 1, as if the generator had to focus 
on a single digit to overcome the faster training of the discriminator. When the generator learning rate was smaller, 
I still observed some amount of mode collapse around 1, but there certainly was more diversity in the digits generated. 
There also were more non-sense results, which may indicate that the generator can fool the discriminator too easily, and thus 
is not trained against proper competition.

## 1.3.3
The FC architecture did not yield good results for CIFAR, even after significant training (>250,000 iterations).
![img]({{ site.baseurl }}/assets/assignment-6/gan-fc-cifar.PNG)

## 1.3.4
Finally, I built a convolutional GAN for CIFAR. I tried to run it over night, but my browser crashed. Still, we get better results for fewer 
iterations than the FC architecture. For example, in the third fake image from the screenshot below, I see some sort of animal, 
possibly a cat sitting on grass. In other words, with this model, shapes can be distinguished even if they're hard to interpret. This 
contrasts with the FC architecture that only seemed to generate images with disconnected color patches.
![img]({{ site.baseurl }}/assets/assignment-6/gan-cnn-cifar.PNG)